{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CAE - Sketch simplification\n\nAttempt sketch simplification using CAE","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"### Libraries","metadata":{}},{"cell_type":"code","source":"# Pytorch libraries\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nimport torch.nn.functional as F\n\n# Torchvision libraries\nimport torchvision.transforms as TF\n\n# open cv\nimport cv2\n\n# pandas library\nimport pandas as pd\n\n# Matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# os library\nimport os\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-22T05:52:11.908262Z","iopub.execute_input":"2023-03-22T05:52:11.908586Z","iopub.status.idle":"2023-03-22T05:52:15.815192Z","shell.execute_reply.started":"2023-03-22T05:52:11.908543Z","shell.execute_reply":"2023-03-22T05:52:15.813753Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Dataset ","metadata":{}},{"cell_type":"code","source":"class Sketches(Dataset):\n    def __init__(self, annotations_file, img_dir, mode = 'train', transform = None, target_transform = None):\n        self.img_labels = annotations_file        # To obtain the labels as the images are annotated in this file\n        self.img_dir = img_dir      # TO obtain the directory of the image\n        self.transform = transform\n        self.target_transform = target_transform\n        self.mode = mode\n        \n    def __len__(self):\n        return len(self.img_labels)  # Returns no. of data through the labels\n    \n    def __getitem__(self, idx):\n        # joining the directory of the images + finding the label with the corresponding idx, and adding its directory name\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx,0])\n        # image reading on the path given: img_path\n        image = cv2.imread(img_path)\n        # Label from the img_labels\n        label = self.img_labels.iloc[idx, 0]\n       \n        # Transforming of the images if needed, eg: converting to Tensors\n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n            label = self.target_transform(label)\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2023-03-22T05:52:15.819973Z","iopub.execute_input":"2023-03-22T05:52:15.820586Z","iopub.status.idle":"2023-03-22T05:52:15.836607Z","shell.execute_reply.started":"2023-03-22T05:52:15.820545Z","shell.execute_reply":"2023-03-22T05:52:15.831967Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameters","metadata":{}},{"cell_type":"code","source":"# Train dataset directory\ntrain_directory = r'/kaggle/input/realsketches'\nprint(f'Train dataset directory: {train_directory}')\n\n# Test dataset directory\ntest_directory = r'/kaggle/input/unclean-sketches'\nprint(f'Test dataset directory: {test_directory}')\n\n# Batch size\nbatch_size = 8\nprint(f'Batch size: {batch_size}')\n\n# Annotations file\nimgs = os.listdir(train_directory)\ntrain_frame = pd.DataFrame(imgs)\n\nimgs = os.listdir(test_directory)\ntest_frame = pd.DataFrame(imgs)\n\n# transform\nimg_transform = TF.Compose([\n            TF.ToTensor(),\n            #TF.Resize((256, 256)),\n            #TF.RandomResizedCrop(256)\n        ])\nprint(f'Transform : {img_transform}')\n\n# Device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Device: {device}')\n\n# Learning rate\nlr = 3e-4\nprint(f'learning rate: {lr}')\n\n# Weight Decay\nwd = 1e-7\nprint(f'Weight Decay: {wd}')\n\n# Bottleneck size\nbottleneck_size = 16\nprint(f'bottleneck width: {bottleneck_size}')\n\n# number of workers\nn_cpu = 2\nprint(f'No. of workers: {n_cpu}')\n\n# No. of epochs\nepochs = 80\nprint(f'No. of epochs: {epochs}')","metadata":{"execution":{"iopub.status.busy":"2023-03-22T05:52:15.838749Z","iopub.execute_input":"2023-03-22T05:52:15.839140Z","iopub.status.idle":"2023-03-22T05:52:16.121828Z","shell.execute_reply.started":"2023-03-22T05:52:15.839093Z","shell.execute_reply":"2023-03-22T05:52:16.120535Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Train dataset directory: /kaggle/input/realsketches\nTest dataset directory: /kaggle/input/unclean-sketches\nBatch size: 8\nTransform : Compose(\n    ToTensor()\n)\nDevice: cuda\nlearning rate: 0.0003\nWeight Decay: 1e-07\nbottleneck width: 16\nNo. of workers: 2\nNo. of epochs: 80\n","output_type":"stream"}]},{"cell_type":"code","source":"test_data = Sketches(\n    img_dir = test_directory,\n    mode = 'test',\n    annotations_file = test_frame,\n    transform = img_transform\n)\n\ntrain_data = Sketches(\n    img_dir = train_directory,\n    mode = 'train',\n    annotations_file = train_frame,\n    transform = img_transform,\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T05:52:16.124634Z","iopub.execute_input":"2023-03-22T05:52:16.129316Z","iopub.status.idle":"2023-03-22T05:52:16.137202Z","shell.execute_reply.started":"2023-03-22T05:52:16.129282Z","shell.execute_reply":"2023-03-22T05:52:16.135633Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Train and Test Dataloader","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:45:37.337519Z","iopub.execute_input":"2023-03-15T11:45:37.338505Z","iopub.status.idle":"2023-03-15T11:45:37.343887Z","shell.execute_reply.started":"2023-03-15T11:45:37.338466Z","shell.execute_reply":"2023-03-15T11:45:37.342777Z"}}},{"cell_type":"code","source":"train_loader = DataLoader(\n    dataset = train_data,\n    batch_size = batch_size,\n    shuffle = True,\n    num_workers = n_cpu\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T05:52:16.139484Z","iopub.execute_input":"2023-03-22T05:52:16.140325Z","iopub.status.idle":"2023-03-22T05:52:16.152387Z","shell.execute_reply.started":"2023-03-22T05:52:16.140286Z","shell.execute_reply":"2023-03-22T05:52:16.147405Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Additional Hyperparameters","metadata":{}},{"cell_type":"code","source":"# No. of batches:\nsize = len(train_loader)\nprint(f'No. of batches: {size}')\n\n# No. of images\ncount = len(os.listdir(train_directory))\nprint(f'No. of images: {count}')\n\nfor batch, (X, _) in enumerate(train_loader):    \n    # Image Size\n    img_size = X[0][0].shape\n    print(f'image size: {img_size}')\n\n    # Channels\n    channels = X[0].shape[0]\n    print(f'No. of channels: {channels}')\n    break","metadata":{"execution":{"iopub.status.busy":"2023-03-22T05:52:16.154530Z","iopub.execute_input":"2023-03-22T05:52:16.155673Z","iopub.status.idle":"2023-03-22T05:52:16.772669Z","shell.execute_reply.started":"2023-03-22T05:52:16.155633Z","shell.execute_reply":"2023-03-22T05:52:16.768279Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"No. of batches: 79\nNo. of images: 632\nimage size: torch.Size([512, 512])\nNo. of channels: 3\n","output_type":"stream"}]},{"cell_type":"code","source":"test_loader = DataLoader(\n    dataset = test_data,\n    batch_size = batch_size,\n    num_workers = 1\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T05:52:16.778042Z","iopub.execute_input":"2023-03-22T05:52:16.780821Z","iopub.status.idle":"2023-03-22T05:52:16.792791Z","shell.execute_reply.started":"2023-03-22T05:52:16.780779Z","shell.execute_reply":"2023-03-22T05:52:16.791740Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"# The input dimensions can be replaced with the dimensions of the image.\nclass Net(torch.nn.Module):\n    def __init__(self, channels = channels):\n        super(Net, self).__init__()\n        self.downconv1 = torch.nn.Sequential(\n            torch.nn.Conv2d(channels, 48, 5, 2, 2),\n            torch.nn.BatchNorm2d(48),\n            torch.nn.ReLU(),\n            \n            torch.nn.Conv2d(48, 128, 3, 1, 1),\n            torch.nn.BatchNorm2d(128),\n            torch.nn.ReLU(),\n            \n            torch.nn.Conv2d(128, 128, 3, 1, 1),\n            torch.nn.BatchNorm2d(128),\n            torch.nn.ReLU(),\n        )\n        self.downconv2 = torch.nn.Sequential(\n            torch.nn.Conv2d(128,256, 3, 2, 1),\n            torch.nn.BatchNorm2d(256),\n            torch.nn.ReLU(),\n            \n            torch.nn.Conv2d(256, 256, 3, 1, 1),\n            torch.nn.BatchNorm2d(256),\n            torch.nn.ReLU(),\n            \n            torch.nn.Conv2d(256, 256, 3, 1, 1),\n            torch.nn.BatchNorm2d(256),\n            torch.nn.ReLU(),\n        )\n        self.downconv3 = torch.nn.Sequential(\n            torch.nn.Conv2d(256, 256, 3, 2, 1),\n            torch.nn.BatchNorm2d(256),\n            torch.nn.ReLU(),\n            \n            torch.nn.Conv2d(256, 512, 3, 1, 1),\n            torch.nn.BatchNorm2d(512),\n            torch.nn.ReLU(),\n            \n            torch.nn.Conv2d(512, 1024, 3, 1, 1),\n            torch.nn.BatchNorm2d(1024),\n            torch.nn.ReLU(),\n        )\n        self.flat = torch.nn.Sequential(\n            torch.nn.Conv2d(1024, 1024, 3, 1, 1),\n            torch.nn.BatchNorm2d(1024),\n            torch.nn.ReLU(),\n            \n            torch.nn.Conv2d(1024, 1024, 3, 1, 1),\n            torch.nn.BatchNorm2d(1024),\n            torch.nn.ReLU(),\n            \n            torch.nn.Conv2d(1024, 1024, 3, 1, 1),\n            torch.nn.BatchNorm2d(1024),\n            torch.nn.ReLU(),\n            \n            torch.nn.Conv2d(1024, 512, 3, 1, 1),\n            torch.nn.BatchNorm2d(512),\n            torch.nn.ReLU(),\n\n            torch.nn.Conv2d(512, 256, 3, 1, 1),\n            torch.nn.BatchNorm2d(256),\n            torch.nn.ReLU(),\n        )\n        \n        self.upconv1 = torch.nn.Sequential(\n#           torch.nn.Conv2d(256, 256, 4, 0.5, 1),\n            torch.nn.ConvTranspose2d(256, 256, 4, 2, 1),\n            torch.nn.BatchNorm2d(256),\n            torch.nn.ReLU(),\n            \n            torch.nn.Conv2d(256, 256, 3, 1, 1),\n            torch.nn.BatchNorm2d(256),\n            torch.nn.ReLU(),\n            \n            torch.nn.Conv2d(256, 128, 3, 1, 1),\n            torch.nn.BatchNorm2d(128),\n            torch.nn.ReLU(),\n        )\n        self.upconv2 = torch.nn.Sequential(\n#             torch.nn.Conv2d(128, 128, 4, 0.5, 1),\n            torch.nn.ConvTranspose2d(128, 128, 4, 2, 1),            \n            torch.nn.BatchNorm2d(128),\n            torch.nn.ReLU(),\n            \n            torch.nn.Conv2d(128, 128, 3, 1, 1),\n            torch.nn.BatchNorm2d(128),\n            torch.nn.ReLU(),\n            \n            torch.nn.Conv2d(128, 48, 3, 1, 1),\n            torch.nn.BatchNorm2d(48),\n            torch.nn.ReLU(),\n        )\n        self.upconv3 = torch.nn.Sequential(\n#             torch.nn.Conv2d(48, 48, 4, 0.5, 1),\n            torch.nn.ConvTranspose2d(48, 48,4, 2, 1),\n            torch.nn.BatchNorm2d(48),\n            torch.nn.ReLU(),\n            \n            torch.nn.Conv2d(48, 24, 3, 1, 1),\n            torch.nn.BatchNorm2d(24),\n            torch.nn.ReLU(),\n            \n            torch.nn.Conv2d(24, channels, 3, 1, 1),\n        )\n\n\n    def forward(self, x):\n        conv1_out = self.downconv1(x)\n        conv2_out = self.downconv2(conv1_out)\n        conv3_out = self.downconv3(conv2_out)\n        flat_out = self.flat(conv3_out)\n        upconv1_out = self.upconv1(flat_out)\n        upconv2_out = self.upconv2(upconv1_out)\n        upconv3_out = self.upconv3(upconv2_out)\n        return upconv3_out","metadata":{"execution":{"iopub.status.busy":"2023-03-22T05:52:16.797941Z","iopub.execute_input":"2023-03-22T05:52:16.798706Z","iopub.status.idle":"2023-03-22T05:52:16.849637Z","shell.execute_reply.started":"2023-03-22T05:52:16.798670Z","shell.execute_reply":"2023-03-22T05:52:16.847620Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"model = Net()\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T05:52:16.856782Z","iopub.execute_input":"2023-03-22T05:52:16.858388Z","iopub.status.idle":"2023-03-22T05:52:17.658925Z","shell.execute_reply.started":"2023-03-22T05:52:16.858352Z","shell.execute_reply":"2023-03-22T05:52:17.656618Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Net(\n  (downconv1): Sequential(\n    (0): Conv2d(3, 48, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n    (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Conv2d(48, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU()\n    (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (8): ReLU()\n  )\n  (downconv2): Sequential(\n    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU()\n    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (8): ReLU()\n  )\n  (downconv3): Sequential(\n    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU()\n    (6): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (8): ReLU()\n  )\n  (flat): Sequential(\n    (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU()\n    (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (8): ReLU()\n    (9): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (11): ReLU()\n    (12): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (14): ReLU()\n  )\n  (upconv1): Sequential(\n    (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU()\n    (6): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (8): ReLU()\n  )\n  (upconv2): Sequential(\n    (0): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU()\n    (6): Conv2d(128, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (8): ReLU()\n  )\n  (upconv3): Sequential(\n    (0): ConvTranspose2d(48, 48, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n    (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Conv2d(48, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU()\n    (6): Conv2d(24, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Loss Function\nloss_fn = nn.BCEWithLogitsLoss()\nprint(loss_fn)\n\n# Optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr = lr, weight_decay = wd)\nprint(optimizer)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T05:52:17.662628Z","iopub.execute_input":"2023-03-22T05:52:17.662917Z","iopub.status.idle":"2023-03-22T05:52:17.670964Z","shell.execute_reply.started":"2023-03-22T05:52:17.662889Z","shell.execute_reply":"2023-03-22T05:52:17.669075Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"BCEWithLogitsLoss()\nAdam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: False\n    lr: 0.0003\n    maximize: False\n    weight_decay: 1e-07\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"def train(dataloader, model, loss_fn, optimizer):\n    model.train()\n    for batch, (X, _) in enumerate(dataloader):\n        # Transforms\n        X, model = X.to(device), model.to(device)\n        \n        # Forwardprop\n        X_reconstructed = model(X)\n        \n        # Flattening\n        flatten_X = X.reshape(batch_size, -1)\n        flatten_X_reconstructed = X_reconstructed.reshape(batch_size, -1)\n                \n        # Calculating loss\n        loss = loss_fn(X_reconstructed, X)\n        \n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # Steps\n        if batch % 4 == 0:\n            loss, current = loss.item(), (batch + 1) * len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{count:>5d}]\")","metadata":{"execution":{"iopub.status.busy":"2023-03-22T05:52:17.672911Z","iopub.execute_input":"2023-03-22T05:52:17.673965Z","iopub.status.idle":"2023-03-22T05:52:17.683355Z","shell.execute_reply.started":"2023-03-22T05:52:17.673928Z","shell.execute_reply":"2023-03-22T05:52:17.682204Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"def Prediction(model, dataloader, epoch):\n    model.eval()\n       \n    with torch.no_grad():    \n        X, _ = next(iter(dataloader))\n        X, model = X.to(device), model.to(device)\n        \n        X_reconstructed = model(X)\n        \n        for j in range(1, batch_size):\n            img = torch.sigmoid(X_reconstructed[j][0]).to('cpu')\n            plt.imsave(f'/kaggle/working/Epoch {epoch} Reconstructed {j} sketch.png', img, cmap = 'gray')\n            ","metadata":{"execution":{"iopub.status.busy":"2023-03-22T05:52:17.685003Z","iopub.execute_input":"2023-03-22T05:52:17.685433Z","iopub.status.idle":"2023-03-22T05:52:17.695783Z","shell.execute_reply.started":"2023-03-22T05:52:17.685385Z","shell.execute_reply":"2023-03-22T05:52:17.694784Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Iteration","metadata":{}},{"cell_type":"code","source":"for i in range(epochs):\n    print(f'Epoch: {i+1} -----------------------')\n    train(train_loader, model, loss_fn, optimizer)\n    if ((i+1) % 5 ==0):\n        Prediction(model, test_loader, i+1)\nprint('Done!')","metadata":{"execution":{"iopub.status.busy":"2023-03-22T05:52:17.697007Z","iopub.execute_input":"2023-03-22T05:52:17.697323Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch: 1 -----------------------\nloss: 0.696778  [    8/  632]\nloss: 0.594334  [   40/  632]\nloss: 0.534555  [   72/  632]\nloss: 0.497206  [  104/  632]\nloss: 0.459505  [  136/  632]\nloss: 0.424979  [  168/  632]\nloss: 0.398229  [  200/  632]\nloss: 0.367586  [  232/  632]\nloss: 0.344488  [  264/  632]\nloss: 0.320195  [  296/  632]\nloss: 0.293789  [  328/  632]\nloss: 0.261584  [  360/  632]\nloss: 0.245224  [  392/  632]\nloss: 0.227931  [  424/  632]\nloss: 0.204851  [  456/  632]\nloss: 0.193476  [  488/  632]\nloss: 0.180564  [  520/  632]\nloss: 0.170256  [  552/  632]\nloss: 0.156621  [  584/  632]\nloss: 0.144217  [  616/  632]\nEpoch: 2 -----------------------\nloss: 0.144435  [    8/  632]\nloss: 0.142909  [   40/  632]\nloss: 0.123221  [   72/  632]\nloss: 0.120119  [  104/  632]\nloss: 0.113414  [  136/  632]\nloss: 0.128983  [  168/  632]\nloss: 0.127224  [  200/  632]\nloss: 0.113000  [  232/  632]\nloss: 0.102027  [  264/  632]\nloss: 0.091174  [  296/  632]\nloss: 0.100461  [  328/  632]\nloss: 0.089190  [  360/  632]\nloss: 0.082804  [  392/  632]\nloss: 0.084906  [  424/  632]\nloss: 0.088724  [  456/  632]\nloss: 0.082108  [  488/  632]\nloss: 0.086666  [  520/  632]\nloss: 0.079453  [  552/  632]\nloss: 0.071249  [  584/  632]\nloss: 0.076391  [  616/  632]\nEpoch: 3 -----------------------\nloss: 0.074431  [    8/  632]\nloss: 0.071447  [   40/  632]\nloss: 0.063511  [   72/  632]\nloss: 0.065325  [  104/  632]\nloss: 0.075831  [  136/  632]\nloss: 0.078696  [  168/  632]\nloss: 0.066339  [  200/  632]\nloss: 0.063822  [  232/  632]\nloss: 0.075182  [  264/  632]\nloss: 0.069380  [  296/  632]\nloss: 0.064898  [  328/  632]\nloss: 0.059912  [  360/  632]\nloss: 0.064120  [  392/  632]\nloss: 0.053536  [  424/  632]\nloss: 0.059951  [  456/  632]\nloss: 0.055132  [  488/  632]\nloss: 0.066917  [  520/  632]\nloss: 0.055380  [  552/  632]\nloss: 0.062617  [  584/  632]\nloss: 0.068370  [  616/  632]\nEpoch: 4 -----------------------\nloss: 0.060094  [    8/  632]\nloss: 0.057839  [   40/  632]\nloss: 0.058862  [   72/  632]\nloss: 0.064638  [  104/  632]\nloss: 0.056157  [  136/  632]\nloss: 0.056562  [  168/  632]\nloss: 0.068091  [  200/  632]\nloss: 0.050441  [  232/  632]\nloss: 0.045654  [  264/  632]\nloss: 0.054214  [  296/  632]\nloss: 0.062519  [  328/  632]\nloss: 0.057021  [  360/  632]\nloss: 0.052075  [  392/  632]\nloss: 0.051602  [  424/  632]\nloss: 0.047708  [  456/  632]\nloss: 0.056738  [  488/  632]\nloss: 0.047374  [  520/  632]\nloss: 0.051558  [  552/  632]\nloss: 0.068454  [  584/  632]\nloss: 0.045862  [  616/  632]\nEpoch: 5 -----------------------\nloss: 0.044055  [    8/  632]\nloss: 0.046550  [   40/  632]\nloss: 0.058157  [   72/  632]\nloss: 0.058174  [  104/  632]\nloss: 0.044488  [  136/  632]\nloss: 0.043974  [  168/  632]\nloss: 0.053117  [  200/  632]\nloss: 0.043912  [  232/  632]\nloss: 0.047850  [  264/  632]\nloss: 0.055001  [  296/  632]\nloss: 0.046310  [  328/  632]\nloss: 0.071447  [  360/  632]\nloss: 0.060352  [  392/  632]\nloss: 0.054919  [  424/  632]\nloss: 0.059280  [  456/  632]\nloss: 0.052282  [  488/  632]\nloss: 0.046192  [  520/  632]\nloss: 0.057023  [  552/  632]\nloss: 0.054073  [  584/  632]\nloss: 0.053893  [  616/  632]\nEpoch: 6 -----------------------\nloss: 0.052266  [    8/  632]\nloss: 0.047019  [   40/  632]\nloss: 0.042586  [   72/  632]\nloss: 0.038115  [  104/  632]\nloss: 0.041801  [  136/  632]\nloss: 0.038969  [  168/  632]\nloss: 0.047001  [  200/  632]\nloss: 0.053478  [  232/  632]\nloss: 0.046714  [  264/  632]\nloss: 0.036271  [  296/  632]\nloss: 0.039944  [  328/  632]\nloss: 0.044079  [  360/  632]\nloss: 0.042778  [  392/  632]\nloss: 0.039008  [  424/  632]\nloss: 0.063485  [  456/  632]\nloss: 0.048687  [  488/  632]\nloss: 0.041808  [  520/  632]\nloss: 0.035138  [  552/  632]\nloss: 0.045895  [  584/  632]\nloss: 0.052382  [  616/  632]\nEpoch: 7 -----------------------\nloss: 0.051631  [    8/  632]\nloss: 0.044619  [   40/  632]\nloss: 0.038676  [   72/  632]\nloss: 0.043197  [  104/  632]\nloss: 0.041215  [  136/  632]\nloss: 0.044929  [  168/  632]\nloss: 0.032908  [  200/  632]\nloss: 0.040038  [  232/  632]\nloss: 0.060622  [  264/  632]\nloss: 0.038320  [  296/  632]\nloss: 0.052138  [  328/  632]\nloss: 0.043735  [  360/  632]\nloss: 0.039598  [  392/  632]\nloss: 0.039431  [  424/  632]\nloss: 0.050481  [  456/  632]\nloss: 0.042259  [  488/  632]\nloss: 0.040126  [  520/  632]\nloss: 0.041378  [  552/  632]\nloss: 0.041810  [  584/  632]\nloss: 0.038249  [  616/  632]\nEpoch: 8 -----------------------\nloss: 0.039177  [    8/  632]\nloss: 0.035219  [   40/  632]\nloss: 0.037826  [   72/  632]\nloss: 0.041302  [  104/  632]\nloss: 0.042770  [  136/  632]\nloss: 0.044540  [  168/  632]\nloss: 0.047524  [  200/  632]\nloss: 0.049227  [  232/  632]\nloss: 0.050629  [  264/  632]\nloss: 0.039704  [  296/  632]\nloss: 0.035620  [  328/  632]\nloss: 0.038540  [  360/  632]\nloss: 0.047314  [  392/  632]\nloss: 0.046669  [  424/  632]\nloss: 0.049187  [  456/  632]\nloss: 0.047650  [  488/  632]\nloss: 0.041482  [  520/  632]\nloss: 0.045545  [  552/  632]\nloss: 0.050418  [  584/  632]\nloss: 0.037216  [  616/  632]\nEpoch: 9 -----------------------\nloss: 0.048686  [    8/  632]\nloss: 0.034502  [   40/  632]\nloss: 0.039247  [   72/  632]\nloss: 0.050021  [  104/  632]\nloss: 0.035157  [  136/  632]\nloss: 0.034926  [  168/  632]\nloss: 0.037887  [  200/  632]\nloss: 0.051562  [  232/  632]\nloss: 0.045925  [  264/  632]\nloss: 0.045185  [  296/  632]\nloss: 0.045498  [  328/  632]\nloss: 0.033264  [  360/  632]\nloss: 0.042785  [  392/  632]\nloss: 0.042302  [  424/  632]\nloss: 0.042525  [  456/  632]\nloss: 0.038865  [  488/  632]\nloss: 0.036328  [  520/  632]\nloss: 0.041259  [  552/  632]\nloss: 0.038136  [  584/  632]\nloss: 0.033658  [  616/  632]\nEpoch: 10 -----------------------\nloss: 0.043554  [    8/  632]\nloss: 0.046192  [   40/  632]\nloss: 0.034502  [   72/  632]\nloss: 0.033105  [  104/  632]\nloss: 0.037906  [  136/  632]\nloss: 0.052327  [  168/  632]\nloss: 0.043056  [  200/  632]\nloss: 0.032854  [  232/  632]\nloss: 0.047320  [  264/  632]\nloss: 0.048876  [  296/  632]\nloss: 0.042927  [  328/  632]\nloss: 0.043232  [  360/  632]\nloss: 0.034144  [  392/  632]\nloss: 0.035741  [  424/  632]\nloss: 0.035202  [  456/  632]\nloss: 0.041134  [  488/  632]\nloss: 0.049708  [  520/  632]\nloss: 0.034518  [  552/  632]\nloss: 0.058050  [  584/  632]\nloss: 0.037329  [  616/  632]\nEpoch: 11 -----------------------\nloss: 0.041395  [    8/  632]\nloss: 0.034735  [   40/  632]\nloss: 0.041254  [   72/  632]\nloss: 0.041123  [  104/  632]\nloss: 0.045698  [  136/  632]\nloss: 0.035664  [  168/  632]\nloss: 0.038955  [  200/  632]\nloss: 0.048256  [  232/  632]\nloss: 0.030831  [  264/  632]\nloss: 0.048157  [  296/  632]\nloss: 0.040583  [  328/  632]\nloss: 0.038388  [  360/  632]\nloss: 0.040650  [  392/  632]\nloss: 0.045028  [  424/  632]\nloss: 0.042699  [  456/  632]\nloss: 0.047042  [  488/  632]\nloss: 0.044676  [  520/  632]\nloss: 0.046926  [  552/  632]\nloss: 0.034998  [  584/  632]\nloss: 0.040248  [  616/  632]\nEpoch: 12 -----------------------\nloss: 0.044421  [    8/  632]\nloss: 0.040102  [   40/  632]\nloss: 0.037621  [   72/  632]\nloss: 0.033239  [  104/  632]\nloss: 0.049224  [  136/  632]\nloss: 0.050737  [  168/  632]\nloss: 0.044064  [  200/  632]\nloss: 0.050726  [  232/  632]\nloss: 0.036704  [  264/  632]\nloss: 0.040979  [  296/  632]\nloss: 0.046116  [  328/  632]\nloss: 0.043518  [  360/  632]\nloss: 0.035168  [  392/  632]\nloss: 0.033074  [  424/  632]\nloss: 0.038765  [  456/  632]\nloss: 0.033042  [  488/  632]\nloss: 0.037709  [  520/  632]\nloss: 0.041391  [  552/  632]\nloss: 0.034904  [  584/  632]\nloss: 0.048806  [  616/  632]\nEpoch: 13 -----------------------\nloss: 0.045067  [    8/  632]\nloss: 0.037531  [   40/  632]\nloss: 0.035385  [   72/  632]\nloss: 0.042996  [  104/  632]\nloss: 0.039664  [  136/  632]\nloss: 0.040597  [  168/  632]\nloss: 0.042316  [  200/  632]\nloss: 0.039246  [  232/  632]\nloss: 0.029542  [  264/  632]\nloss: 0.040545  [  296/  632]\nloss: 0.037643  [  328/  632]\nloss: 0.040715  [  360/  632]\nloss: 0.033213  [  392/  632]\nloss: 0.038162  [  424/  632]\nloss: 0.057602  [  456/  632]\nloss: 0.035838  [  488/  632]\nloss: 0.046089  [  520/  632]\nloss: 0.046045  [  552/  632]\nloss: 0.042791  [  584/  632]\nloss: 0.051019  [  616/  632]\nEpoch: 14 -----------------------\nloss: 0.034606  [    8/  632]\nloss: 0.033879  [   40/  632]\nloss: 0.043470  [   72/  632]\nloss: 0.040051  [  104/  632]\nloss: 0.036221  [  136/  632]\nloss: 0.037512  [  168/  632]\nloss: 0.034651  [  200/  632]\nloss: 0.041306  [  232/  632]\nloss: 0.042703  [  264/  632]\nloss: 0.035758  [  296/  632]\nloss: 0.044920  [  328/  632]\nloss: 0.034932  [  360/  632]\nloss: 0.043056  [  392/  632]\nloss: 0.031117  [  424/  632]\nloss: 0.048947  [  456/  632]\nloss: 0.041604  [  488/  632]\nloss: 0.046787  [  520/  632]\nloss: 0.035598  [  552/  632]\nloss: 0.036827  [  584/  632]\nloss: 0.034418  [  616/  632]\nEpoch: 15 -----------------------\nloss: 0.042171  [    8/  632]\nloss: 0.038278  [   40/  632]\nloss: 0.024772  [   72/  632]\nloss: 0.035066  [  104/  632]\nloss: 0.046604  [  136/  632]\nloss: 0.042380  [  168/  632]\nloss: 0.029200  [  200/  632]\nloss: 0.049513  [  232/  632]\nloss: 0.039743  [  264/  632]\nloss: 0.042277  [  296/  632]\nloss: 0.026410  [  328/  632]\nloss: 0.037534  [  360/  632]\nloss: 0.034247  [  392/  632]\nloss: 0.044017  [  424/  632]\nloss: 0.046828  [  456/  632]\nloss: 0.041601  [  488/  632]\nloss: 0.031353  [  520/  632]\nloss: 0.041961  [  552/  632]\nloss: 0.043004  [  584/  632]\nloss: 0.030279  [  616/  632]\nEpoch: 16 -----------------------\nloss: 0.039975  [    8/  632]\nloss: 0.041079  [   40/  632]\nloss: 0.035340  [   72/  632]\nloss: 0.036718  [  104/  632]\nloss: 0.031695  [  136/  632]\nloss: 0.040323  [  168/  632]\nloss: 0.040653  [  200/  632]\nloss: 0.045744  [  232/  632]\nloss: 0.023575  [  264/  632]\nloss: 0.046266  [  296/  632]\nloss: 0.040497  [  328/  632]\nloss: 0.037689  [  360/  632]\nloss: 0.046322  [  392/  632]\nloss: 0.037727  [  424/  632]\nloss: 0.042043  [  456/  632]\nloss: 0.046902  [  488/  632]\nloss: 0.031006  [  520/  632]\nloss: 0.030500  [  552/  632]\nloss: 0.041306  [  584/  632]\nloss: 0.045271  [  616/  632]\nEpoch: 17 -----------------------\nloss: 0.035588  [    8/  632]\nloss: 0.038446  [   40/  632]\nloss: 0.037475  [   72/  632]\nloss: 0.036635  [  104/  632]\nloss: 0.036561  [  136/  632]\nloss: 0.053716  [  168/  632]\nloss: 0.037408  [  200/  632]\nloss: 0.042886  [  232/  632]\nloss: 0.033104  [  264/  632]\nloss: 0.039126  [  296/  632]\nloss: 0.041042  [  328/  632]\nloss: 0.038073  [  360/  632]\nloss: 0.041452  [  392/  632]\nloss: 0.033628  [  424/  632]\nloss: 0.041010  [  456/  632]\nloss: 0.043292  [  488/  632]\nloss: 0.042851  [  520/  632]\nloss: 0.034152  [  552/  632]\nloss: 0.042031  [  584/  632]\nloss: 0.033731  [  616/  632]\nEpoch: 18 -----------------------\nloss: 0.037936  [    8/  632]\nloss: 0.044775  [   40/  632]\nloss: 0.052230  [   72/  632]\nloss: 0.040474  [  104/  632]\nloss: 0.035975  [  136/  632]\nloss: 0.045983  [  168/  632]\nloss: 0.031528  [  200/  632]\nloss: 0.038131  [  232/  632]\nloss: 0.031785  [  264/  632]\nloss: 0.044892  [  296/  632]\nloss: 0.037550  [  328/  632]\nloss: 0.033707  [  360/  632]\nloss: 0.032251  [  392/  632]\nloss: 0.046958  [  424/  632]\nloss: 0.038208  [  456/  632]\nloss: 0.040379  [  488/  632]\nloss: 0.047468  [  520/  632]\nloss: 0.035019  [  552/  632]\nloss: 0.040106  [  584/  632]\nloss: 0.039197  [  616/  632]\nEpoch: 19 -----------------------\nloss: 0.045101  [    8/  632]\nloss: 0.038178  [   40/  632]\nloss: 0.036991  [   72/  632]\nloss: 0.048098  [  104/  632]\nloss: 0.037366  [  136/  632]\nloss: 0.034557  [  168/  632]\nloss: 0.036862  [  200/  632]\nloss: 0.034985  [  232/  632]\nloss: 0.045044  [  264/  632]\nloss: 0.046045  [  296/  632]\nloss: 0.034229  [  328/  632]\nloss: 0.042276  [  360/  632]\nloss: 0.034350  [  392/  632]\nloss: 0.035782  [  424/  632]\nloss: 0.034708  [  456/  632]\nloss: 0.036415  [  488/  632]\nloss: 0.040200  [  520/  632]\nloss: 0.033630  [  552/  632]\nloss: 0.036555  [  584/  632]\nloss: 0.040660  [  616/  632]\nEpoch: 20 -----------------------\nloss: 0.035661  [    8/  632]\nloss: 0.040850  [   40/  632]\nloss: 0.035642  [   72/  632]\nloss: 0.038502  [  104/  632]\nloss: 0.039698  [  136/  632]\nloss: 0.038856  [  168/  632]\nloss: 0.041829  [  200/  632]\nloss: 0.033679  [  232/  632]\nloss: 0.035105  [  264/  632]\nloss: 0.034479  [  296/  632]\nloss: 0.035602  [  328/  632]\nloss: 0.033623  [  360/  632]\nloss: 0.039779  [  392/  632]\nloss: 0.043326  [  424/  632]\nloss: 0.040474  [  456/  632]\nloss: 0.046882  [  488/  632]\nloss: 0.025914  [  520/  632]\nloss: 0.034875  [  552/  632]\nloss: 0.049739  [  584/  632]\nloss: 0.031220  [  616/  632]\nEpoch: 21 -----------------------\nloss: 0.038267  [    8/  632]\nloss: 0.033748  [   40/  632]\nloss: 0.047696  [   72/  632]\nloss: 0.041016  [  104/  632]\nloss: 0.036586  [  136/  632]\nloss: 0.042654  [  168/  632]\nloss: 0.045794  [  200/  632]\nloss: 0.037645  [  232/  632]\nloss: 0.037487  [  264/  632]\nloss: 0.039328  [  296/  632]\nloss: 0.030202  [  328/  632]\nloss: 0.044532  [  360/  632]\nloss: 0.041848  [  392/  632]\nloss: 0.030884  [  424/  632]\nloss: 0.034893  [  456/  632]\nloss: 0.037520  [  488/  632]\nloss: 0.038894  [  520/  632]\nloss: 0.041300  [  552/  632]\nloss: 0.036861  [  584/  632]\nloss: 0.034634  [  616/  632]\nEpoch: 22 -----------------------\nloss: 0.032819  [    8/  632]\nloss: 0.038418  [   40/  632]\nloss: 0.033333  [   72/  632]\nloss: 0.042324  [  104/  632]\nloss: 0.032497  [  136/  632]\nloss: 0.043562  [  168/  632]\nloss: 0.038963  [  200/  632]\nloss: 0.032153  [  232/  632]\nloss: 0.035420  [  264/  632]\nloss: 0.029391  [  296/  632]\nloss: 0.044219  [  328/  632]\nloss: 0.029635  [  360/  632]\nloss: 0.035919  [  392/  632]\nloss: 0.036923  [  424/  632]\nloss: 0.033525  [  456/  632]\nloss: 0.032957  [  488/  632]\nloss: 0.038800  [  520/  632]\nloss: 0.037955  [  552/  632]\nloss: 0.040867  [  584/  632]\nloss: 0.035182  [  616/  632]\nEpoch: 23 -----------------------\nloss: 0.036472  [    8/  632]\nloss: 0.036762  [   40/  632]\nloss: 0.035773  [   72/  632]\nloss: 0.044363  [  104/  632]\nloss: 0.040083  [  136/  632]\nloss: 0.033385  [  168/  632]\nloss: 0.034676  [  200/  632]\nloss: 0.040118  [  232/  632]\nloss: 0.031043  [  264/  632]\nloss: 0.035523  [  296/  632]\nloss: 0.045890  [  328/  632]\nloss: 0.038513  [  360/  632]\nloss: 0.039539  [  392/  632]\nloss: 0.036688  [  424/  632]\nloss: 0.040207  [  456/  632]\nloss: 0.035385  [  488/  632]\nloss: 0.041477  [  520/  632]\nloss: 0.048076  [  552/  632]\nloss: 0.031402  [  584/  632]\nloss: 0.037260  [  616/  632]\nEpoch: 24 -----------------------\nloss: 0.038726  [    8/  632]\nloss: 0.035334  [   40/  632]\nloss: 0.045362  [   72/  632]\nloss: 0.034309  [  104/  632]\nloss: 0.033633  [  136/  632]\nloss: 0.034584  [  168/  632]\nloss: 0.036313  [  200/  632]\nloss: 0.030215  [  232/  632]\nloss: 0.037338  [  264/  632]\nloss: 0.035851  [  296/  632]\nloss: 0.028994  [  328/  632]\nloss: 0.039021  [  360/  632]\nloss: 0.031618  [  392/  632]\nloss: 0.046902  [  424/  632]\nloss: 0.034197  [  456/  632]\nloss: 0.039644  [  488/  632]\nloss: 0.041256  [  520/  632]\nloss: 0.043694  [  552/  632]\nloss: 0.036231  [  584/  632]\nloss: 0.039242  [  616/  632]\nEpoch: 25 -----------------------\nloss: 0.030303  [    8/  632]\nloss: 0.038211  [   40/  632]\nloss: 0.045683  [   72/  632]\nloss: 0.035342  [  104/  632]\nloss: 0.031498  [  136/  632]\nloss: 0.033249  [  168/  632]\nloss: 0.033350  [  200/  632]\nloss: 0.037691  [  232/  632]\nloss: 0.038799  [  264/  632]\nloss: 0.031931  [  296/  632]\nloss: 0.039489  [  328/  632]\nloss: 0.030855  [  360/  632]\nloss: 0.038770  [  392/  632]\nloss: 0.028615  [  424/  632]\nloss: 0.034934  [  456/  632]\nloss: 0.029919  [  488/  632]\nloss: 0.032294  [  520/  632]\nloss: 0.041502  [  552/  632]\nloss: 0.044789  [  584/  632]\nloss: 0.039240  [  616/  632]\nEpoch: 26 -----------------------\nloss: 0.041520  [    8/  632]\nloss: 0.034178  [   40/  632]\nloss: 0.035300  [   72/  632]\nloss: 0.038644  [  104/  632]\nloss: 0.022383  [  136/  632]\nloss: 0.033962  [  168/  632]\nloss: 0.041316  [  200/  632]\nloss: 0.038333  [  232/  632]\nloss: 0.046879  [  264/  632]\nloss: 0.036796  [  296/  632]\nloss: 0.048396  [  328/  632]\nloss: 0.034536  [  360/  632]\nloss: 0.044521  [  392/  632]\nloss: 0.031012  [  424/  632]\nloss: 0.037576  [  456/  632]\nloss: 0.040853  [  488/  632]\nloss: 0.039974  [  520/  632]\nloss: 0.038835  [  552/  632]\nloss: 0.042149  [  584/  632]\nloss: 0.029434  [  616/  632]\nEpoch: 27 -----------------------\nloss: 0.040896  [    8/  632]\nloss: 0.024286  [   40/  632]\nloss: 0.041379  [   72/  632]\nloss: 0.036886  [  104/  632]\nloss: 0.041244  [  136/  632]\nloss: 0.047037  [  168/  632]\nloss: 0.038098  [  200/  632]\nloss: 0.049884  [  232/  632]\nloss: 0.049598  [  264/  632]\nloss: 0.030302  [  296/  632]\nloss: 0.043063  [  328/  632]\nloss: 0.048471  [  360/  632]\nloss: 0.035042  [  392/  632]\nloss: 0.034504  [  424/  632]\nloss: 0.034746  [  456/  632]\nloss: 0.039021  [  488/  632]\nloss: 0.041033  [  520/  632]\nloss: 0.038383  [  552/  632]\nloss: 0.026457  [  584/  632]\nloss: 0.038582  [  616/  632]\nEpoch: 28 -----------------------\nloss: 0.034262  [    8/  632]\nloss: 0.031344  [   40/  632]\nloss: 0.038377  [   72/  632]\nloss: 0.041968  [  104/  632]\nloss: 0.032679  [  136/  632]\nloss: 0.033730  [  168/  632]\nloss: 0.035723  [  200/  632]\nloss: 0.035702  [  232/  632]\nloss: 0.033061  [  264/  632]\nloss: 0.034127  [  296/  632]\nloss: 0.035159  [  328/  632]\nloss: 0.045946  [  360/  632]\nloss: 0.036120  [  392/  632]\nloss: 0.039671  [  424/  632]\nloss: 0.031445  [  456/  632]\nloss: 0.039697  [  488/  632]\nloss: 0.032567  [  520/  632]\nloss: 0.042232  [  552/  632]\nloss: 0.050224  [  584/  632]\nloss: 0.041248  [  616/  632]\nEpoch: 29 -----------------------\nloss: 0.037939  [    8/  632]\nloss: 0.038644  [   40/  632]\nloss: 0.033580  [   72/  632]\nloss: 0.032227  [  104/  632]\nloss: 0.030562  [  136/  632]\nloss: 0.036532  [  168/  632]\nloss: 0.037030  [  200/  632]\nloss: 0.036899  [  232/  632]\nloss: 0.033576  [  264/  632]\nloss: 0.030396  [  296/  632]\nloss: 0.047149  [  328/  632]\nloss: 0.034752  [  360/  632]\nloss: 0.028946  [  392/  632]\nloss: 0.036713  [  424/  632]\nloss: 0.034704  [  456/  632]\nloss: 0.040914  [  488/  632]\nloss: 0.037962  [  520/  632]\nloss: 0.048354  [  552/  632]\nloss: 0.038246  [  584/  632]\nloss: 0.035400  [  616/  632]\nEpoch: 30 -----------------------\nloss: 0.040936  [    8/  632]\nloss: 0.035957  [   40/  632]\nloss: 0.033148  [   72/  632]\nloss: 0.031700  [  104/  632]\nloss: 0.040841  [  136/  632]\nloss: 0.032346  [  168/  632]\nloss: 0.035931  [  200/  632]\nloss: 0.036918  [  232/  632]\nloss: 0.040029  [  264/  632]\nloss: 0.040625  [  296/  632]\nloss: 0.043493  [  328/  632]\nloss: 0.043000  [  360/  632]\nloss: 0.040898  [  392/  632]\nloss: 0.045594  [  424/  632]\nloss: 0.042642  [  456/  632]\nloss: 0.038200  [  488/  632]\nloss: 0.045720  [  520/  632]\nloss: 0.033673  [  552/  632]\nloss: 0.042459  [  584/  632]\nloss: 0.042170  [  616/  632]\nEpoch: 31 -----------------------\nloss: 0.027559  [    8/  632]\nloss: 0.031925  [   40/  632]\nloss: 0.037479  [   72/  632]\nloss: 0.036708  [  104/  632]\nloss: 0.034758  [  136/  632]\nloss: 0.035224  [  168/  632]\nloss: 0.040996  [  200/  632]\nloss: 0.048905  [  232/  632]\nloss: 0.042305  [  264/  632]\nloss: 0.033895  [  296/  632]\nloss: 0.037124  [  328/  632]\nloss: 0.038205  [  360/  632]\nloss: 0.034901  [  392/  632]\nloss: 0.035897  [  424/  632]\nloss: 0.038765  [  456/  632]\nloss: 0.044008  [  488/  632]\nloss: 0.045990  [  520/  632]\nloss: 0.036263  [  552/  632]\nloss: 0.042393  [  584/  632]\nloss: 0.038625  [  616/  632]\nEpoch: 32 -----------------------\nloss: 0.041506  [    8/  632]\nloss: 0.030441  [   40/  632]\nloss: 0.032260  [   72/  632]\nloss: 0.036507  [  104/  632]\nloss: 0.034212  [  136/  632]\nloss: 0.037754  [  168/  632]\nloss: 0.027505  [  200/  632]\nloss: 0.035126  [  232/  632]\nloss: 0.039423  [  264/  632]\nloss: 0.039264  [  296/  632]\nloss: 0.036509  [  328/  632]\nloss: 0.034660  [  360/  632]\nloss: 0.030645  [  392/  632]\nloss: 0.030482  [  424/  632]\nloss: 0.029783  [  456/  632]\nloss: 0.038588  [  488/  632]\nloss: 0.043058  [  520/  632]\nloss: 0.039067  [  552/  632]\nloss: 0.040997  [  584/  632]\nloss: 0.035088  [  616/  632]\nEpoch: 33 -----------------------\nloss: 0.040834  [    8/  632]\nloss: 0.027896  [   40/  632]\nloss: 0.032813  [   72/  632]\nloss: 0.030399  [  104/  632]\nloss: 0.031068  [  136/  632]\nloss: 0.040504  [  168/  632]\nloss: 0.032882  [  200/  632]\nloss: 0.028761  [  232/  632]\nloss: 0.042085  [  264/  632]\nloss: 0.033932  [  296/  632]\nloss: 0.035454  [  328/  632]\nloss: 0.034681  [  360/  632]\nloss: 0.031331  [  392/  632]\nloss: 0.040071  [  424/  632]\nloss: 0.031200  [  456/  632]\nloss: 0.040214  [  488/  632]\nloss: 0.036164  [  520/  632]\nloss: 0.045838  [  552/  632]\nloss: 0.041879  [  584/  632]\nloss: 0.034660  [  616/  632]\n","output_type":"stream"}]},{"cell_type":"code","source":"Prediction(model, test_loader, 'final')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
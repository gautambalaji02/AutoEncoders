{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Convolutional VAE\nWe will work on the sketch dataset \n\n### Libraries","metadata":{"execution":{"iopub.status.busy":"2023-03-10T08:33:29.452236Z","iopub.execute_input":"2023-03-10T08:33:29.452635Z","iopub.status.idle":"2023-03-10T08:33:29.670568Z","shell.execute_reply.started":"2023-03-10T08:33:29.452601Z","shell.execute_reply":"2023-03-10T08:33:29.669395Z"}}},{"cell_type":"code","source":"# Pytorch libraries\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nimport torch.nn.functional as F\n\n# Torchvision libraries\nimport torchvision.transforms as TF\n\n# open cv\nimport cv2\n\n# pandas library\nimport pandas as pd\n\n# Matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# os library\nimport os\n","metadata":{"execution":{"iopub.status.busy":"2023-03-14T11:08:51.648124Z","iopub.execute_input":"2023-03-14T11:08:51.648605Z","iopub.status.idle":"2023-03-14T11:08:54.587070Z","shell.execute_reply.started":"2023-03-14T11:08:51.648564Z","shell.execute_reply":"2023-03-14T11:08:54.585629Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Dataset\nWe will create our own custom dataset","metadata":{}},{"cell_type":"code","source":"class Sketches(Dataset):\n    def __init__(self, annotations_file, img_dir, mode = 'train', transform = None, target_transform = None):\n        self.img_labels = annotations_file        # To obtain the labels as the images are annotated in this file\n        self.img_dir = img_dir      # TO obtain the directory of the image\n        self.transform = transform\n        self.target_transform = target_transform\n        self.mode = mode\n        \n    def __len__(self):\n        return len(self.img_labels)  # Returns no. of data through the labels\n    \n    def __getitem__(self, idx):\n        # joining the directory of the images + finding the label with the corresponding idx, and adding its directory name\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx,0])\n        # image reading on the path given: img_path\n        image = cv2.imread(img_path)\n        # Label from the img_labels\n        label = self.img_labels.iloc[idx, 0]\n       \n        # Transforming of the images if needed, eg: converting to Tensors\n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n            label = self.target_transform(label)\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2023-03-14T11:08:54.589182Z","iopub.execute_input":"2023-03-14T11:08:54.589730Z","iopub.status.idle":"2023-03-14T11:08:54.598950Z","shell.execute_reply.started":"2023-03-14T11:08:54.589693Z","shell.execute_reply":"2023-03-14T11:08:54.597686Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Creating an instance of the dataset class","metadata":{}},{"cell_type":"markdown","source":"### Hyperparameters","metadata":{}},{"cell_type":"code","source":"# Dataset directory\ndirectory = r'/kaggle/input/realsketches'\nprint(f'Dataset directory: {directory}')\n\n# Batch size\nbatch_size = 16\nprint(f'Batch size: {batch_size}')\n\n# Annotations file\nimgs = os.listdir(directory)\nframe = pd.DataFrame(imgs)\n\n# transform\nimg_transform = TF.Compose([\n            TF.ToTensor(),\n            TF.Resize((256, 256)),\n            TF.RandomResizedCrop(256)\n        ])\nprint(f'Transform : {img_transform}')\n\n# Device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Device: {device}')\n\n# Learning rate\nlr = 3e-4\nprint(f'learning rate: {lr}')\n\n# Weight Decay\nwd = 1e-7\nprint(f'Weight Decay: {wd}')\n\n# Bottleneck size\nbottleneck_size = 16\nprint(f'bottleneck width: {bottleneck_size}')\n\n# number of workers\nn_cpu = 2\nprint(f'No. of workers: {n_cpu}')\n\n# No. of epochs\nepochs = 40\nprint(f'No. of epochs: {epochs}')","metadata":{"execution":{"iopub.status.busy":"2023-03-14T11:08:54.601258Z","iopub.execute_input":"2023-03-14T11:08:54.601834Z","iopub.status.idle":"2023-03-14T11:08:54.687931Z","shell.execute_reply.started":"2023-03-14T11:08:54.601783Z","shell.execute_reply":"2023-03-14T11:08:54.686519Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Dataset directory: /kaggle/input/realsketches\nBatch size: 16\nTransform : Compose(\n    ToTensor()\n    Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=None)\n    RandomResizedCrop(size=(256, 256), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear), antialias=None)\n)\nDevice: cpu\nlearning rate: 0.0003\nWeight Decay: 1e-07\nbottleneck width: 16\nNo. of workers: 2\nNo. of epochs: 40\n","output_type":"stream"}]},{"cell_type":"code","source":"data = Sketches(\n    img_dir = directory,\n    mode = 'train',\n    annotations_file = frame,\n    transform = img_transform,\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-14T11:08:54.691268Z","iopub.execute_input":"2023-03-14T11:08:54.691969Z","iopub.status.idle":"2023-03-14T11:08:54.698721Z","shell.execute_reply.started":"2023-03-14T11:08:54.691926Z","shell.execute_reply":"2023-03-14T11:08:54.697339Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### DataLoader","metadata":{}},{"cell_type":"code","source":"loader = DataLoader(\n    dataset = data,\n    batch_size = batch_size,\n    shuffle = True,\n    num_workers = n_cpu\n)\n\n# No. of batches:\nsize = len(loader)\nprint(f'No. of batches: {size}')\n\n# No. of images\ncount = len(os.listdir(directory))\nprint(f'No. of images: {count}')","metadata":{"execution":{"iopub.status.busy":"2023-03-14T11:08:54.700686Z","iopub.execute_input":"2023-03-14T11:08:54.701431Z","iopub.status.idle":"2023-03-14T11:08:54.714575Z","shell.execute_reply.started":"2023-03-14T11:08:54.701379Z","shell.execute_reply":"2023-03-14T11:08:54.712504Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"No. of batches: 40\nNo. of images: 632\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Additional Hyperparameters","metadata":{}},{"cell_type":"code","source":"for batch, (X, _) in enumerate(loader):    \n    # Image Size\n    img_size = X[0][0].shape\n    print(f'image size: {img_size}')\n\n    # Channels\n    channels = X[0].shape[0]\n    print(f'No. of channels: {channels}')\n    break","metadata":{"execution":{"iopub.status.busy":"2023-03-14T11:08:54.717443Z","iopub.execute_input":"2023-03-14T11:08:54.718440Z","iopub.status.idle":"2023-03-14T11:08:55.495038Z","shell.execute_reply.started":"2023-03-14T11:08:54.718389Z","shell.execute_reply":"2023-03-14T11:08:55.493524Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"image size: torch.Size([256, 256])\nNo. of channels: 3\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Model\nNow that we have our dataloader and dataset, we cna move on to creating our model","metadata":{"execution":{"iopub.status.busy":"2023-03-10T10:11:50.707561Z","iopub.execute_input":"2023-03-10T10:11:50.708307Z","iopub.status.idle":"2023-03-10T10:11:50.714121Z","shell.execute_reply.started":"2023-03-10T10:11:50.708264Z","shell.execute_reply":"2023-03-10T10:11:50.712695Z"}}},{"cell_type":"code","source":"class CVAE(nn.Module):\n    def __init__(self, img_size = img_size, channels = channels, h_dim = 64, z_dim = bottleneck_size ):\n        super(CVAE, self).__init__()\n        \n        # used in encoder\n        self.ENconv_block = nn.Sequential(\n            nn.Conv2d(\n                in_channels = channels,\n                out_channels = h_dim,\n                kernel_size = 3,\n                stride = 2,\n                padding = 1\n            ),\n            nn.ReLU(),\n            nn.BatchNorm2d(h_dim),\n            nn.Conv2d(\n                in_channels = h_dim,\n                out_channels = int(h_dim / 2),\n                kernel_size = 3,\n                stride = 1,\n                padding = 1\n            ),\n            nn.MaxPool2d(2),\n            nn.ReLU(),\n            nn.BatchNorm2d(int(h_dim / 2))\n        )\n        \n        # Used in Decoder\n        self.DEconv_block = nn.Sequential(\n            nn.Conv2d(\n                in_channels = int(bottleneck_size),\n                out_channels = int(h_dim / 2),\n                stride = 1,\n                kernel_size = 3,\n                padding = 1\n            ),\n            nn.ReLU(),\n            nn.Upsample(scale_factor = 2),\n            nn.Conv2d(\n                in_channels = int(h_dim / 2),\n                out_channels = int(h_dim),\n                stride = 1,\n                kernel_size = 3,\n                padding = 1\n            ),\n            nn.ReLU(),\n            nn.Upsample(scale_factor = 2),\n            nn.Conv2d(\n                in_channels = h_dim,\n                out_channels = channels,\n                stride = 1,\n                kernel_size = 3,\n                padding = 1\n            ),         \n            nn.Sigmoid()\n        )\n        \n        # Mu and Sigma\n        self.Mu_block = nn.Conv2d(\n                in_channels = int(h_dim / 2),\n                out_channels = bottleneck_size,\n                stride = 1,\n                kernel_size = 3,\n                padding = 1\n            )\n        \n        self.Sigma_block = nn.Conv2d(\n                in_channels = int(h_dim / 2),\n                out_channels = bottleneck_size,\n                stride = 1,\n                kernel_size = 3,\n                padding = 1\n            )\n        \n    def encoder(self, x):\n        z = self.ENconv_block(x)\n        mu, sigma = self.Mu_block(z), self.Sigma_block(z)\n        return mu, sigma\n    \n    def decoder(self, z):\n        x_reconstructed = self.DEconv_block(z)\n        return x_reconstructed\n    \n    def forward(self, data):\n        # Encoding\n        mu, sigma = self.encoder(data)\n        \n        # Reparameterization\n        eps = torch.rand_like(sigma)\n        z_reparameterized = mu + ( sigma * eps )\n        \n        # Decoding\n        x_reconstructed = self.decoder(z_reparameterized)\n        \n        return x_reconstructed, mu, sigma\n\n# Model\nmodel = CVAE()\nmodel = model.to(device)\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2023-03-14T11:08:55.496786Z","iopub.execute_input":"2023-03-14T11:08:55.497282Z","iopub.status.idle":"2023-03-14T11:08:55.530079Z","shell.execute_reply.started":"2023-03-14T11:08:55.497219Z","shell.execute_reply":"2023-03-14T11:08:55.528932Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"CVAE(\n  (ENconv_block): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): ReLU()\n    (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (DEconv_block): Sequential(\n    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Upsample(scale_factor=2.0, mode=nearest)\n    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): ReLU()\n    (5): Upsample(scale_factor=2.0, mode=nearest)\n    (6): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): Sigmoid()\n  )\n  (Mu_block): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (Sigma_block): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Loss and optimizer functions","metadata":{}},{"cell_type":"code","source":"# Loss Function\nloss_fn = nn.BCELoss()\nprint(loss_fn)\n\n# Optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr = lr, weight_decay = wd)\nprint(optimizer)","metadata":{"execution":{"iopub.status.busy":"2023-03-14T11:08:55.531220Z","iopub.execute_input":"2023-03-14T11:08:55.531526Z","iopub.status.idle":"2023-03-14T11:08:55.538495Z","shell.execute_reply.started":"2023-03-14T11:08:55.531497Z","shell.execute_reply":"2023-03-14T11:08:55.537180Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"BCELoss()\nAdam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: False\n    lr: 0.0003\n    maximize: False\n    weight_decay: 1e-07\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Train Function","metadata":{}},{"cell_type":"code","source":"def train(dataloader, model, loss_fn, optimizer):\n    model.train()\n    for batch, (X, _) in enumerate(dataloader):\n        # Transforms\n        X = X.to(device)\n        \n        # Forwardprop\n        X_reconstructed, mu, sigma = model(X)\n        \n        # Flattening\n        flatten_X = X.reshape(batch_size, -1)\n        flatten_X_reconstructed = X_reconstructed.reshape(batch_size, -1)\n                \n        # Calculating loss\n        loss = loss_fn(flatten_X_reconstructed, flatten_X)\n        \n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # Steps\n        if batch % 4 == 0:\n            loss, current = loss.item(), (batch + 1) * len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{count:>5d}]\")","metadata":{"execution":{"iopub.status.busy":"2023-03-14T11:08:55.539922Z","iopub.execute_input":"2023-03-14T11:08:55.540857Z","iopub.status.idle":"2023-03-14T11:08:55.551187Z","shell.execute_reply.started":"2023-03-14T11:08:55.540819Z","shell.execute_reply":"2023-03-14T11:08:55.549759Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Prediction","metadata":{}},{"cell_type":"code","source":"def Prediction(model, dataloader):\n    model.eval()\n       \n    with torch.no_grad():    \n        X, _ = next(iter(dataloader))\n        X, model = X.to(device), model.to(device)\n        \n        flatten_X = X.reshape(batch_size, -1)\n        flatten_X_reconstructed, mu, sigma = model(flatten_X)\n        \n        X_reconstructed = X.reshape(batch_size, img_size[0], img_size[1])\n        \n        for j in range(1, batch_size):\n            plt.imsave(f'Reconstructed {j} sketch', X_reconstructed[j])\n            ","metadata":{"execution":{"iopub.status.busy":"2023-03-14T11:08:55.557174Z","iopub.execute_input":"2023-03-14T11:08:55.557611Z","iopub.status.idle":"2023-03-14T11:08:55.566021Z","shell.execute_reply.started":"2023-03-14T11:08:55.557558Z","shell.execute_reply":"2023-03-14T11:08:55.564699Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Model iteration","metadata":{}},{"cell_type":"code","source":"for i in range(epochs):\n    print(f'Epoch: {i+1} -----------------------')\n    train(loader, model, loss_fn, optimizer)\nprint('Done!')\nPrediction(model, loader)","metadata":{"execution":{"iopub.status.busy":"2023-03-14T11:08:55.567698Z","iopub.execute_input":"2023-03-14T11:08:55.568769Z","iopub.status.idle":"2023-03-14T11:09:04.610644Z","shell.execute_reply.started":"2023-03-14T11:08:55.568713Z","shell.execute_reply":"2023-03-14T11:09:04.608591Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Epoch: 1 -----------------------\nloss: 0.666950  [   16/  632]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/1974111813.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch: {i+1} -----------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mPrediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_27/1345326482.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[1;32m    487\u001b[0m         torch.autograd.backward(\n\u001b[0;32m--> 488\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         )\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m def grad(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}